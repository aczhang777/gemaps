---
title: "gemaps dataset"
output: html_document
---
confusion matrices, accuracy rates, true positives and true negatives: https://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/
setwd("/Users/anniezhang/Desktop/Research/Anderson")
```{r}
save.image(file = "gemaps.RData")
```

```{r}
load("gemaps.RData")
```

Libraries:
```{r}
library(caret)
library(randomForest)
```
Preliminary: data cleaning
```{r}
gemaps_dataset <- read.csv("gemaps_updated.csv", header=TRUE, sep=",")
nrow(gemaps_dataset) ## 3528 rows
ncol(gemaps_dataset) ## 66 cols

## this is an empty data frame that we're going to fill
gemaps_dataset_nozeroes <- data.frame(matrix(ncol=ncol(gemaps_dataset), nrow=nrow(gemaps_dataset)))
colnames(gemaps_dataset_nozeroes) <- colnames(gemaps_dataset)

## this is to get rid of any row that has all zero entries.
k <- 1 
for(i in 1:nrow(gemaps_dataset)){
  delete <- T
  j <- 5
  while (delete == TRUE && j <= 66) {
    if (gemaps_dataset[i,j] != 0) {
      delete <- FALSE
    }
    j <- j + 1
  }
  if (delete == FALSE) {
    gemaps_dataset_nozeroes[k,] <- gemaps_dataset[i,]
    k <- k + 1
  }
}

## this is to get rid of duplicates: this is the dataset we will actually use for our analysis!
gemaps <- unique(gemaps_dataset_nozeroes)
colnames(gemaps) <- colnames(gemaps_dataset)
attach(gemaps)
```

Logistic model:
```{r}
all <- glm(factor(has_family_with_autism) ~ . - email - name - frameTime, data = gemaps, family = "binomial")
summary(all)
```
From the logistic model, we get that these variables are significant at the 0.05 level: F0semitoneFrom27.5Hz_sma3nz_meanFallingSlope, logRelF0.H1.H2_sma3nz_stddevNorm, hammarbergIndexV_sma3nz_stddevNorm, slopeV500.1500_sma3nz_amean, StddevVoicedSegmentLengthSec.
These are significant at the 0.1 level:
logRelF0.H1.A3_sma3nz_stddevNorm and F2frequency_sma3nz_amean.

K-Fold Cross Validation on the logistic model:
Procedure:
1. divide data into 10 folds of equal size
2. build logistic model on 9 folds, test on remaining fold
3. record error rate
4. repeat
```{r}
set.seed(626) ## for reproduceability
gemaps1 <- gemaps[sample(nrow(gemaps)),] ## shuffle rows
folds <- cut(seq(1, nrow(gemaps)), breaks = 10, labels = FALSE) ## divide rows into 10 folds
emptyList <- vector("list", 4)
confusionMatricesLog <- list(rep(emptyList, times = 10)) ## storing all of our confusion matrices + add'l info here
for (i in 1:10) {
  testIndices <- which(folds == i, arr.ind = TRUE)
  testingData <- gemaps1[testIndices, ]
  trainingData <- gemaps1[-testIndices, ]
  logModel <- glm(has_family_with_autism ~ . - email - name - frameTime, data = trainingData, family = "binomial")
  predictedValues <- round(predict(object = logModel, newdata = testingData, type = "response")) ## type = "response" gives us responses in terms of probabilities, not log odds. I also rounded the values so I could create the confusion matrices.
  confusionMatricesLog[[i]] <- (confusionMatrix(data = as.factor(predictedValues), reference = as.factor(testingData$has_family_with_autism), positive = "1"))
}
```
From here we can calculate accuracy rates by class for each of the 10 folds:
```{r}
confusionMatricesLog[[1]]$table
```
When we test on the first fold, we get that the error rate for the class "has family with autism" is 100%, while for "doesn't have family with autism", it's approximately 5/326 = 1.533%. Overall error rate: 22/343 = 0.0641, or 6.41%. 

```{r}
confusionMatricesLog[[2]]$table
```
When we test on the second, error rate for has family w/autism = 100%, and for family without, it's 1/327 = 0.305%. Overall error rate: 16/342 = 4.67%.

```{r}
confusionMatricesLog[[3]]$table
```
Third: error rate for family with autism = 18/19 = 94.737%, while for family without, it's 0%. Overall error rate: 18/342 = 5.26%.

```{r}
confusionMatricesLog[[4]]$table
```
Fourth: error rate for family with autism = 100%. Without: 0%. Overall error rate: 12/342 = 3.51%.

```{r}
confusionMatricesLog[[5]]$table
```
Fifth: error rate for family w/autism = 100%. Without: 1/321 = 0.312%. Overall error rate: 22/342 = 6.43%.

```{r}
confusionMatricesLog[[6]]$table
```
Sixth: error rate for family w/autism = 100%. Without = 0%. Overall: 23/342 = 6.73%.

```{r}
confusionMatricesLog[[7]]$table
```
7th: error rate for family w/autism = 100%. Without: 1/321 = 0.312%. Overall: 23/342 = 6.73%.

```{r}
confusionMatricesLog[[8]]$table
```
8th: error rate for family w/autism = 100%. Without: 0%. Overall: 19/342 = 5.56%.

```{r}
confusionMatricesLog[[9]]$table
```
9th: error rate for family w/autism = 100%. Without: 0%. Overall: 26/342 = 7.60%.

```{r}
confusionMatricesLog[[10]]$table
```
10th: error rate for family w/autism = 100%. Without: 0%. Overall: 16/342 = 4.68%.

What is the mean and standard deviation of the error rates under a logistic model (by class)? What is the mean and standard deviation of the overall error rate?
```{r}
meanAtRiskLog <- mean(c(rep(100, times = 9), 94.737))
meanNotAtRiskLog <- mean(c(1.533, 0.305, 0.312, 0.312, rep(0, times = 6)))
sdAtRiskLog <- sd(c(rep(100, times = 9), 94.737))
sdNotAtRiskLog <- sd(c(1.533, 0.305, 0.312, 0.312, rep(0, times = 6)))
meanOverallLog <- mean(c(6.41, 4.67, 5.26, 3.51, 6.43, 6.73, 6.73, 5.56, 7.6, 4.68))
sdOverallLog <- sd(c(6.41, 4.67, 5.26, 3.51, 6.43, 6.73, 6.73, 5.56, 7.6, 4.68))
meanAtRiskLog;meanNotAtRiskLog;sdAtRiskLog;sdNotAtRiskLog;meanOverallLog;sdOverallLog
```
Based on these error rates, these variables are probably not good for discerning whether or not someone has a family member with autism.

Now let's look at random forests:

Procedure:
1. get 10 folds
2. test on remaining 9 folds.
3. get error rate.
4. repeat.
```{r}
set.seed(515)
gemaps2 <- gemaps[sample(nrow(gemaps)),]
foldsRF <- cut(seq(1, nrow(gemaps)), breaks = 10, labels = FALSE)
list14 <- vector("list", 14)
randFor <- list(rep(list14, times = 10))
for (i in 1:10) {
  testIndicesRF <- which(folds == i, arr.ind = TRUE)
  testingDataRF <- gemaps2[testIndicesRF,]
  trainingDataRF <- gemaps2[-testIndicesRF,]
  randFor[[i]] <- randomForest(factor(has_family_with_autism) ~ . - email - name - frameTime, data = trainingDataRF, importance = TRUE, keep.forest = TRUE)
}
```

We'll follow the same procedure as we did for logistic regression to determine the error rates for each class in each fold and the mean and standard deviation of the error rates for each class.
```{r}
randFor[[1]]$confusion
```
For at risk for autism, the error rate is 100%. For not at risk, it's 1.582%. The overall error rate is (46 + 171) / (46 + 171 + 2861) = 7.05%.

```{r}
randFor[[2]]$confusion
```
For at risk, the error rate is 100%. Not at risk is 1.546%. Overall: (45 + 169) / (45 + 169 + 2865) = 6.95%.

```{r}
randFor[[3]]$confusion
```
At risk: 100%. Not at risk: 1.516%. Overall: (44 + 177) / (44 + 177 + 2858) = 7.18%.

```{r}
randFor[[4]]$confusion
```
At risk: 100%. Not at risk: 1.687%. Overall: (49 + 175) / (49 + 175 + 2855) = 7.28%.

```{r}
randFor[[5]]$confusion
```
At risk: 100%. Not at risk: 1.718%. Overall: 220 / (220 + 2859) = 7.15%.

```{r}
randFor[[6]]$confusion
```
At risk: 100%. Not at risk: 1.620%. Overall: (178 + 47) / (178 + 47 + 2854) = 7.31%.

```{r}
randFor[[7]]$confusion
```
At risk: 100%. Not at risk: 1.820%. Overall: (168 + 53) / (168 + 53 + 2858) = 7.18%.

```{r}
randFor[[8]]$confusion
```
At risk: 100%. Not at risk: 1.545%. Overall: (166 + 45) / (166 + 45 + 2868) = 6.85%.

```{r}
randFor[[9]]$confusion
```
At risk: 100%. Not at risk: 1.615%. Overall: (169 + 47) / (169 + 47 + 2863) = 7.02%.

```{r}
randFor[[10]]$confusion
```
At risk: 100%. Not at risk: 1.442%. Overall: (167 + 42) / (167 + 42 + 2870) = 6.79%.

Now let's find the means and standard deviations of these class error rates:
```{r}
meanAtRiskRF <- 100
meanNotAtRiskRF <- mean(c(1.442, 1.615, 1.545, 1.820, 1.620, 1.718, 1.687, 1.516, 1.546, 1.582))
sdAtRiskRF <- 0
sdNotAtRiskRF <- sd(c(1.442, 1.615, 1.545, 1.820, 1.620, 1.718, 1.687, 1.516, 1.546, 1.582))
meanOverallRF <- mean(c(7.05, 6.95, 7.18, 7.28, 7.15, 7.31, 7.18, 6.85, 7.02, 6.79))
sdOverallRF <- sd(c(7.05, 6.95, 7.18, 7.28, 7.15, 7.31, 7.18, 6.85, 7.02, 6.79))
meanAtRiskRF;meanNotAtRiskRF;sdAtRiskRF;sdNotAtRiskRF;meanOverallRF;sdOverallRF
```
Based on these error rates, random forests also doesn't seem to be great at discerning people who are at risk for autism.
